# Object-Detection-Eval

Чтобы воспользоваться скриптом для оценки качества модели, надо просто запустить `python model_detection_eval.py` с аргументами:
- "-h", который показывает help-message
- "-t" и указать в кавычках через пробел значения трешхолда для IOU, которые надо использовать (ex.: '-t "0.5 0.6 0.9"'), по дефолту будет использоваться только 0.5
- "-gt_path", чтобы указать путь к файлу с ground truth 
- "-pred_path", чтобы указать путь к файлу с предиктами модели
- "-output_path", чтобы указать в какой файл записать результаты теста моделей

Пример ввода:
`python model_detection_eval.py -gt_path val_small_40_classes.json -pred_path coco_instances_results.json -output_path output.txt -t "0.5 0.6 0.9"`

В результирующем файлике для каждого ввыбранного нами порога IOU будет AP по классам, Accuracy по классам, а также усредненный mAP для данных порогов и по IOU=[.5 .. .95], усредненный Accuracy и AR (усредненный по IOU=[.5 .. .95]). Пример выхода лежит в файлике `output.txt`.
